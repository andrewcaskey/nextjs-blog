---
Id: 1004
Title: IOPS vs Throughput vs Latency 
Author: Andrew Caskey
Tags: Networking
Topic: Cloud Computing
Abstract: Going over the key differences and similarities of IOPS and throughput with examples 
HeaderImage: /BL-1004/hero.jpeg
isPublished: true
---

Welcome to this blog post about storage metrics! In this post, we'll be discussing two important measurements in the world of storage: IOPS and throughput. By understanding the differences between these two metrics, you can gain a better understanding of how your storage system is performing. We'll also touch on a third important metric, latency, and how it factors into storage performance.



## IOPS {#IOPS}

Input Output operations per second

IOPS are storage measurement of the number of I/O operations per second. IOPS is frequently referenced by storage vendors to characterize performance in solid-state drives (SSD), hard disk drives (HDD) and storage area networks. However, an IOPS number is not an actual benchmark, and numbers promoted by vendors may not correspond to real-world performance.

![IOPS](/BL-1004/IOPS.png)



## Throughput {#Throughput}

Throguhput measures how many units of information a system can process in a period of time. Large files are read from beginning to end and hence throughput operations are measured in Mega Byte per second (MB/s). This storage metric describes the amount of data able to flow through a point in the data path over a given time. Throughput is typically the best storage metric when measuring data that needs to be streamed rapidly, such as images and video files.



## IOPS vs Throughput {#IOPS-vs-Throughput}

To summarize the difference in throughput vs. IOPS, IOPS is a count of the read/write operations per second, but throughput is the actual measurement of read/write bits per second that are transferred over a network. 

Here is a good example I found on [stack overflow](https://stackoverflow.com/questions/15759571/iops-versus-throughput) going over the key differences between the two, 

Think of it as:

You have 4 buckets (Disk blocks) of the same size that you want to fill or empty with water.

You'll be using a jug to transfer the water into the buckets. Now your question will be:

At a given time (per second), how many jugs of water can you pour (write) or withdraw (read)? This is IOPS.

At a given time (per second) what's the amount (bit, kb, mb, etc) of water the jug can transfer into/out of the bucket continuously? This is throughput.

Additionally, there is a delay in the process of you pouring and/or withdrawing the water. This is Latency.

There's 3 things to consider when talking about IOPS and Throughput:

Size (file size/block size)
Patterns (Random/Sequential)
Mix (Read/Write) percentage




## Latency {#Latency}
Latency is the time between request to storage and the response received. Once this process is completed, latency also includes the time it takes find the required data blocks and to prepare to transfer data.

Latency is measured in units of time. 

With regards to IOPS, latency is a measure of the length of time it takes for a single I/O request to be completed from the application's point of view.

While not providing a complete picture, combining latency, IOPS and throughput measurements can help gauge performance.
